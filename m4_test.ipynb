{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ESRNN.M4_data import prepare_M4_data\n",
    "from ESRNN.ESRNN import ESRNN\n",
    "\n",
    "from ESRNN.ESRNNensemble import ESRNNensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded M4-info.csv 4335598 bytes.\n",
      "Successfully downloaded Train/Daily-train.csv 95765153 bytes.\n",
      "Successfully downloaded Train/Hourly-train.csv 2347115 bytes.\n",
      "Successfully downloaded Train/Monthly-train.csv 91655432 bytes.\n",
      "Successfully downloaded Train/Quarterly-train.csv 38788547 bytes.\n",
      "Successfully downloaded Train/Weekly-train.csv 4015067 bytes.\n",
      "Successfully downloaded Train/Yearly-train.csv 25355736 bytes.\n",
      "Successfully downloaded Test/Daily-test.csv 576459 bytes.\n",
      "Successfully downloaded Test/Hourly-test.csv 132820 bytes.\n",
      "Successfully downloaded Test/Monthly-test.csv 7942698 bytes.\n",
      "Successfully downloaded Test/Quarterly-test.csv 1971754 bytes.\n",
      "Successfully downloaded Test/Weekly-test.csv 44247 bytes.\n",
      "Successfully downloaded Test/Yearly-test.csv 1486434 bytes.\n",
      "\n",
      "\n",
      "Preparing Yearly dataset\n",
      "Preparing Naive2 Yearly dataset predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y1</td>\n",
       "      <td>1970-01-02</td>\n",
       "      <td>5172.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y1</td>\n",
       "      <td>1970-01-03</td>\n",
       "      <td>5133.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y1</td>\n",
       "      <td>1970-01-04</td>\n",
       "      <td>5186.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y1</td>\n",
       "      <td>1970-01-05</td>\n",
       "      <td>5084.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y1</td>\n",
       "      <td>1970-01-06</td>\n",
       "      <td>5182.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Y9</td>\n",
       "      <td>1970-01-16</td>\n",
       "      <td>2124.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Y9</td>\n",
       "      <td>1970-01-17</td>\n",
       "      <td>2134.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Y9</td>\n",
       "      <td>1970-01-18</td>\n",
       "      <td>2150.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Y9</td>\n",
       "      <td>1970-01-19</td>\n",
       "      <td>2184.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Y9</td>\n",
       "      <td>1970-01-20</td>\n",
       "      <td>2290.372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id         ds         y\n",
       "0          Y1 1970-01-02  5172.100\n",
       "1          Y1 1970-01-03  5133.500\n",
       "2          Y1 1970-01-04  5186.900\n",
       "3          Y1 1970-01-05  5084.600\n",
       "4          Y1 1970-01-06  5182.000\n",
       "..        ...        ...       ...\n",
       "226        Y9 1970-01-16  2124.897\n",
       "227        Y9 1970-01-17  2134.967\n",
       "228        Y9 1970-01-18  2150.078\n",
       "229        Y9 1970-01-19  2184.454\n",
       "230        Y9 1970-01-20  2290.372\n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df, y_train_df, X_test_df, y_test_df = prepare_M4_data('Yearly', directory = './data', num_obs=10) #, num_obs=5\n",
    "y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ESRNN  0\n",
      "=============== Training ESRNN  ===============\n",
      "\n",
      "========= Epoch 0 finished =========\n",
      "Training time: 5.32868\n",
      "Training loss: 0.2983599901199341\n",
      "panel_delta [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\n",
      "unit self.mc.frequency D\n",
      "OWA: 3.029 \n",
      "SMAPE: 22.725 \n",
      "MASE: 3.792 \n",
      "Train finished! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "## Weekly\n",
    "# esrnn = ESRNN(max_epochs=4, batch_size=4, dilations=[[1, 4], [52]], rnn_weight_decay=0.5, freq_of_test=1)\n",
    "\n",
    "## Quarterly\n",
    "# esrnn = ESRNN(max_epochs=1, batch_size=3, rnn_weight_decay=0.5, freq_of_test=1)\n",
    "\n",
    "## Daily\n",
    "# esrnn = ESRNN(max_epochs=2, batch_size=16, learning_rate=3e-4, per_series_lr_multip=1.5,\n",
    "#               gradient_eps=1e-6, level_variability_penalty=100,\n",
    "#               dilations=[[1,7],[28]], add_nl_layer=True,\n",
    "#               seasonality=[7], input_size=7, output_size=14)\n",
    "\n",
    "# # Debugging Quarterly\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     esrnn = ESRNN(max_epochs=1, max_periods=1, batch_size=3, state_hsize=4,\n",
    "#                   seasonality=[1], input_size=3, output_size=8, dilations=[[1, 2]],\n",
    "#                   rnn_weight_decay=0.0, freq_of_test=1, random_seed=1, cell_type='LSTM')\n",
    "#     esrnn.fit(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "\n",
    "# # Proof of Concept Quarterly\n",
    "# esrnn = ESRNN(max_epochs=2, max_periods=20, batch_size=1, state_hsize=40,\n",
    "#               learning_rate = 0.001, per_series_lr_multip=1.5,\n",
    "#               seasonality=[], input_size=4, output_size=8, dilations=[[1, 2], [4, 8]],\n",
    "#               rnn_weight_decay=0.0, freq_of_test=1, random_seed=1, cell_type='ResLSTM')\n",
    "# esrnn.fit(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "\n",
    "# # Debugging Quarterly\n",
    "# print(\"\\n\\n\")\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     esrnn = ESRNN(max_epochs=1, max_periods=1, batch_size=10, state_hsize=4, per_series_lr_multip=1.5,\n",
    "#                   seasonality=[1, 2], input_size=3, output_size=8, dilations=[[1, 2]],\n",
    "#                   rnn_weight_decay=0.0, freq_of_test=1, random_seed=1, cell_type='LSTM',\n",
    "#                   level_variability_penalty=0)\n",
    "#     esrnn.fit(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "    \n",
    "# # Debugging Yearly\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     esrnn = ESRNN(max_epochs=1, max_periods=4, batch_size=1, state_hsize=6,\n",
    "#                   seasonality=[4], input_size=3, output_size=6, dilations=[[1, 2],[3]],\n",
    "#                   rnn_weight_decay=1.0, freq_of_test=1, random_seed=1, cell_type='LSTM') #AttentiveLSTM\n",
    "#     esrnn.fit(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "    \n",
    "# Debugging Yearly\n",
    "with torch.autograd.detect_anomaly():\n",
    "    esrnn = ESRNN_ensemble(max_epochs=1, max_periods=4, batch_size=1, state_hsize=6,\n",
    "                           seasonality=[4], input_size=3, output_size=6, dilations=[[1, 2],[3]], num_splits=1,\n",
    "                           rnn_weight_decay=1.0, freq_of_test=1, random_seed=1, cell_type='LSTM') #AttentiveLSTM\n",
    "    esrnn.fit(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "\n",
    "## Debugging Hourly\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     esrnn = ESRNN(max_epochs=4, max_periods=1, batch_size=3, state_hsize=4,\n",
    "#                   seasonality=[24, 168], input_size=3, output_size=48, dilations=[[1, 2]],\n",
    "#                   rnn_weight_decay=1.0, freq_of_test=1, random_seed=1, cell_type='LSTM')\n",
    "#     esrnn.fit(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "\n",
    "# # Debugging Weekly\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     esrnn = ESRNN(max_epochs=1, max_periods=1, batch_size=3, state_hsize=4,\n",
    "#                   seasonality=[52], input_size=10, output_size=13, dilations=[[1, 52]],\n",
    "#                   rnn_weight_decay=1.0, freq_of_test=1, random_seed=1, cell_type='LSTM')\n",
    "#     esrnn.fit(X_train_df, y_train_df, X_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = esrnn.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_prediction(y_train_df, X_test_df, y_test_df, model=esrnn, u_id='Q1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esrnn.evaluate_model_prediction(y_train_df, X_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M4 eval\n",
    "#(0.778*(23.0/100) + 0.847*(24.0/100) + 0.836*(48.0/100) + 0.920*(5.0/100))\n",
    "#(13.176*() + 9.679*() + 12.126*())\n",
    "#(14.42*(23.0/(23+24+48)) + 10.09*(24.0/(23+24+48)) + 10.81*(48.0/(23+24+48)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hyperpar_tunning_m4 import parse_grid_search\n",
    "from src.utils_visualization import plot_grid_cat_distributions\n",
    "\n",
    "gs_df = parse_grid_search('Daily')\n",
    "# plot_cat_distributions(df=gs_df, cat='learning_rate', var='min_owa')\n",
    "# plot_cat_distributions(df=gs_df, cat='add_nl_layer', var='min_owa')\n",
    "# plot_cat_distributions(df=gs_df, cat='rnn_weight_decay', var='min_owa')\n",
    "# plot_cat_distributions(df=gs_df, cat='per_series_lr_multip', var='min_owa')\n",
    "# plot_cat_distributions(df=gs_df, cat='batch_size', var='min_owa')\n",
    "# plot_cat_distributions(df=gs_df, cat='training_percentile', var='min_owa')\n",
    "# plot_cat_distributions(df=gs_df, cat='dilations', var='min_owa')\n",
    "\n",
    "#gs_df['early_stopping'] = gs_df.min_epoch < gs_df.max_epochs\n",
    "#gs_df = gs_df[gs_df.min_owa<1.5]\n",
    "plot_grid_cat_distributions(gs_df, var = 'min_owa',\n",
    "                            cats=['cell_type', 'learning_rate', 'per_series_lr_multip',\n",
    "                                  'batch_size', 'max_epochs', 'max_periods', 'dilations',\n",
    "                                  'rnn_weight_decay'])\n",
    "#gs_df.learning_rate.min()\n",
    "# fgs_df = gs_df[gs_df.rnn_weight_decay==0.0]\n",
    "fgs_df = gs_df\n",
    "#fgs_df.groupby('rnn_weight_decay')['min_owa'].mean()\n",
    "# 'batch_size' : [8, 16]\n",
    "# 'learning_rate': [3e-4, 5e-4, 1.0e-3, 1.5e-3]\n",
    "# 'per_series_lr_multip': [1.5, 2.0, 3.0]\n",
    "# 'training_percentile' : [45, 50]\n",
    "# 'max_periods': [10, 15]\n",
    "# 'cell_type': ['LSTM']\n",
    "# 'dilations' : [[[1, 7, 28]]]\n",
    "# 'ensemble': [True]\n",
    "# 'add_nl_layer' : [False],\n",
    "# 'random_seed': [1, 2, 117, 120652, 117982, 1210357]\n",
    "# gs_df.min_owa.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "DAILY = {'model_type': ['esrnn'],\n",
    "         'dataset': ['Daily'],\n",
    "         'max_epochs' : [30],\n",
    "         'batch_size' : [8, 32],\n",
    "         'freq_of_test': [4],\n",
    "         'learning_rate': [3e-4, 5e-4, 1.0e-3, 1.5e-3],\n",
    "         'lr_scheduler_step_size' : [9],\n",
    "         'lr_decay' : [0.333],\n",
    "         'per_series_lr_multip': [0.5, 1.0, 1.5, 2.0, 3.0],\n",
    "         'gradient_clipping_threshold' : [50],\n",
    "         'rnn_weight_decay' : [0.05, 0.01],\n",
    "         'noise_std' : [1e-3],\n",
    "         'level_variability_penalty' : [100],\n",
    "         'percentile' : [50],\n",
    "         'training_percentile' : [45],\n",
    "         'max_periods': [10, 15],\n",
    "         'cell_type': ['LSTM', 'ResLSTM'],\n",
    "         'state_hsize' : [40],\n",
    "         'dilations' : [[[1, 7, 28]]],\n",
    "         'add_nl_layer' : [True],\n",
    "         'seasonality' : [[7]],\n",
    "         'input_size' : [7],\n",
    "         'output_size' : [14],\n",
    "         'random_seed': [1, 2, 117, 120652, 117982, 1210357],\n",
    "         'device' : ['cuda']}\n",
    "\n",
    "model_specs = DAILY\n",
    "\n",
    "specs_list = list(itertools.product(*list(model_specs.values())))\n",
    "model_specs_df = pd.DataFrame(specs_list,\n",
    "                        columns=list(model_specs.keys()))\n",
    "\n",
    "len(model_specs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver2 = ver[ver.y_hat.isnull()]\n",
    "ver2 = ver[y_test_df.unique_id=='W1']\n",
    "ver3 = y_test_df[y_test_df.unique_id=='W2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = gs_df[gs_df.min_owa == gs_df.min_owa.min()]\n",
    "for x in ver:\n",
    "    print(ver[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.M4_data import prepare_M4_data\n",
    "from src.utils_evaluation import evaluate_prediction_owa\n",
    "\n",
    "from src.ESRNN import ESRNN\n",
    "\n",
    "X_train_df, y_train_df, X_test_df, y_test_df = prepare_M4_data(dataset_name='Yearly', num_obs=23000)\n",
    "\n",
    "# Instantiate model\n",
    "model = ESRNN(max_epochs=25,\n",
    "              batch_size=4,\n",
    "              freq_of_test=5,\n",
    "              learning_rate=1e-4,\n",
    "              lr_scheduler_step_size=10,\n",
    "              lr_decay=0.1,\n",
    "              per_series_lr_multip=0.8,\n",
    "              gradient_clipping_threshold=50,\n",
    "              rnn_weight_decay=0.0,\n",
    "              noise_std=0.001,\n",
    "              level_variability_penalty=100,\n",
    "              percentile=50,\n",
    "              training_percentile=50,\n",
    "              ensemble=False,\n",
    "              max_periods=25,\n",
    "              seasonality=[],\n",
    "              input_size=4,\n",
    "              output_size=6,\n",
    "              cell_type='LSTM',\n",
    "              state_hsize=40,\n",
    "              dilations=[[1], [6]],\n",
    "              add_nl_layer=False,\n",
    "              random_seed=1,\n",
    "              device='cpu')\n",
    "\n",
    "# Fit model\n",
    "# If y_test_df is provided the model will evaluate predictions on this set every freq_test epochs\n",
    "model.fit(X_train_df, y_train_df, X_test_df, y_test_df)\n",
    "\n",
    "# Predict on test set\n",
    "y_hat_df = model.predict(X_test_df)\n",
    "\n",
    "# Evaluate predictions\n",
    "print(15*'=', ' Final evaluation ', 14*'=')\n",
    "final_owa, final_mase, final_smape = evaluate_prediction_owa(y_hat_df, y_train_df, \n",
    "                                                             X_test_df, y_test_df,\n",
    "                                                             naive2_seasonality=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_i.permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.785*23000+0.879*24000+0.872*48000+0.97*5000)/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esrnn_torch",
   "language": "python",
   "name": "esrnn_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
